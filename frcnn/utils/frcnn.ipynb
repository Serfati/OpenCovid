{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Mask Detection with Faster R-CNN (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-662b919cc15f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DIR_INPUT = \"/storage/users/Ise4thYear/OpenCoVid/files/rcnn/data/\"\n",
    "DIR_IMAGES = DIR_INPUT + \"/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Dataset\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Null Values, Unique Values\n",
    "\n",
    "unq_values = df[\"name\"].unique()\n",
    "print(\"Total Records: \", len(df))\n",
    "print(\"Unique Images: \",len(unq_values))\n",
    "\n",
    "null_values = df.isnull().sum(axis = 0)\n",
    "print(\"\\n> Null Values in each column <\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Total Classes\n",
    "\n",
    "classes = df[\"class\"].unique()\n",
    "print(\"Total Classes: \",len(classes))\n",
    "print(\"\\n> Classes <\\n\",classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing Class Distribution\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.title('Class Distribution', fontsize= 20)\n",
    "sns.countplot(x = \"classname\", data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize image with bounding box for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to plot image\n",
    "\n",
    "def plot_img(image_name):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize = (14, 14))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    bbox = df[df['name'] == image_name]\n",
    "    img_path = os.path.join(DIR_IMAGES, image_name)\n",
    "    \n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image2 = image\n",
    "    \n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].imshow(image)\n",
    "    \n",
    "    for idx, row in bbox.iterrows():\n",
    "        x1 = row['xmin']\n",
    "        y1 = row['ymin']\n",
    "        x2 = row['xmax']\n",
    "        y2 = row['ymax']\n",
    "        label = row['classname']\n",
    "        \n",
    "        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 3)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, (255,0,0), 2)\n",
    "    \n",
    "    ax[1].set_title('Image with Boundary Box')\n",
    "    ax[1].imshow(image2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pass any image name as parameter\n",
    "\n",
    "plot_img(\"-1x-1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class <-> Int\n",
    "\n",
    "_classes = np.insert(classes, 0, \"background\", axis=0)        # adding a background class for Faster R-CNN\n",
    "class_to_int = {_classes[i] : i for i in range(len(_classes))}\n",
    "int_to_class = {i : _classes[i] for i in range(len(_classes))}\n",
    "print(\"class_to_int : \\n\",class_to_int)\n",
    "print(\"\\nint_to_class : \\n\",int_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Data (Labels & Targets) for Faster R-CNN\n",
    "\n",
    "class FaceMaskDetectionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, image_dir, mode = 'train', transforms = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_names = dataframe[\"name\"].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        #Retrive Image name and its records (x1, y1, x2, y2, class) from df\n",
    "        image_name = self.image_names[index]\n",
    "        records = self.df[self.df[\"name\"] == image_name]\n",
    "        \n",
    "        #Loading Image\n",
    "        image = cv2.imread(self.image_dir + image_name, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            \n",
    "            #Get bounding box co-ordinates for each box\n",
    "            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "\n",
    "            #Getting labels for each box\n",
    "            temp_labels = records[['class']].values\n",
    "            labels = []\n",
    "            for label in temp_labels:\n",
    "                label = class_to_int[label[0]]\n",
    "                labels.append(label)\n",
    "\n",
    "            #Converting boxes & labels into torch tensor\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            #Creating target\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "\n",
    "            #Transforms\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "\n",
    "\n",
    "            return image, target, image_name\n",
    "        \n",
    "        elif self.mode == 'test':\n",
    "\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "\n",
    "            return image, image_name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform for Train & Valid\n",
    "\n",
    "## Using Albumentations\n",
    "#def get_transform():\n",
    "    #return A.Compose([\n",
    "        #ToTensorV2(p=1.0)\n",
    "    #], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "## Using torchvision.transforms - without Augmentation!\n",
    "def get_transform():\n",
    "    return T.Compose([T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing data for Train & Validation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "#Dataset object\n",
    "dataset = FaceMaskDetectionDataset(df, DIR_IMAGES, transforms = get_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set - using 80% for training, 20% for validation\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "range = len(dataset) * 0.2\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:-range])\n",
    "valid_dataset = torch.utils.data.Subset(dataset, indices[-range:])\n",
    "\n",
    "\n",
    "#Preparing data loaders\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model - Resnet50 (Faster R-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use GPU if possible, otherwise use CPU\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create / load model\n",
    "\n",
    "#Faster - RCNN Model - pretrained on COCO dataset\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = len(class_to_int) # 2: with_mask or without_mask\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Training - Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriving all trainable parameters from model (for optimizer)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "#Defininig Optimizer\n",
    "#optimizer = torch.optim.Adam(params, lr = 0.0001)\n",
    "optimizer = torch.optim.SGD(params, lr = 0.005, momentum = 0.9)\n",
    "\n",
    "#LR\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "#Load pre-trained model\n",
    "# checkpoint = torch.load(\"fmd_frcnn_e23.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#No of epochs\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 1\n",
    "total_train_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    \n",
    "    #Retriving Mini-batch\n",
    "    for images, targets, image_names in train_data_loader:\n",
    "        \n",
    "        #Loading images & targets on device\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        #Forward propagation\n",
    "        out = model(images, targets)\n",
    "        losses = sum(loss for loss in out.values())\n",
    "        \n",
    "        #Reseting Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Back propagation\n",
    "        losses.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Average loss\n",
    "        loss_value = losses.item()\n",
    "        train_loss.append(loss_value)\n",
    "        \n",
    "        if itr % 25 == 0:\n",
    "            print(f\"\\n Iteration #{itr} loss: {out} \\n\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    #lr_scheduler.step()    \n",
    "    \n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    total_train_loss.append(epoch_train_loss)\n",
    "    print(f'Epoch train loss is {epoch_train_loss:.4f}')\n",
    "\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(\"Time elapsed: \",time_elapsed)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_train_loss\n",
    "            }, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Details**\n",
    "\n",
    "* Architecture = ResNet50\n",
    "* Method = Faster R-CNN\n",
    "* Pretrained Weights = MS_COCO\n",
    "* Learning Rate = 0.05\n",
    "* Optimizer = SGD with momentum 0.9\n",
    "* Epochs = 25\n",
    "* Avg Time per Epoch (80% Train Data) = 690 sec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Details (After 30 Epochs of Training)**\n",
    "\n",
    "* cls_loss     =  0.0466\n",
    "* reg_loss     =  0.0125\n",
    "* obj_loss     =  0.0040\n",
    "* rpn_reg_loss =  0.0120\n",
    "* Overall      =  0.0546"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}